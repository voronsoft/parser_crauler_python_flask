### Парсер - краулер задание: 

Задача:  
Реализовать web-crawler, рекурсивно скачивающий сайт (идущий по ссылкам в глубь).  
completed - **1.** Crawler должен скачать документ по указанному URL и продолжить закачку по ссылкам, находящимся в документе.  
completed - **2.** Crawler должен поддерживать - до закачку.  
completed - **3.** Crawler должен грузить только текстовые документы - __html, css, js, json (игнорировать картинки, видео, и пр.)__  
completed - **4.** Crawler должен грузить документы только одного домена __(игнорировать сторонние ссылки)__  
completed - **5.** Crawler должен быть __многопоточным__ (какие именно части параллелить - полностью ваше решение)  

Требования специально даны неформально.  
Мы хотим увидеть, как вы по постановке задаче самостоятельно примете решение, что более важно, а что менее.

На выходе мы ожидаем работающее приложение, которое сможем собрать и запустить.  
Мы не ожидаем правильной обработки всех типов ошибок и граничных случаев, вы сами себе должны поставить отсечку "__good enough__".  

PS.
Ограничений по 3rd-party библиотекам нет.

